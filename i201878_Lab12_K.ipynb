{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55ab6f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba42f937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis(X,thetas):\n",
    "    return np.dot(X,thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d126959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fun(y_act,y_pred):\n",
    "    return np.mean((y_act-y_pred)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f962b22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def der_loss_fun(X, y, thetas):\n",
    "    h = hypothesis(X,thetas)\n",
    "    dif = (y-h).T\n",
    "    f=np.dot(dif,X)\n",
    "    f=-2*f/float(X.shape[0])\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bd63ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(nthetas, alpha, grad_thetas):\n",
    "    return (nthetas -alpha * grad_thetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b79ef71",
   "metadata": {},
   "source": [
    "# Task-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5719b5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>radio</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230.1</td>\n",
       "      <td>37.8</td>\n",
       "      <td>69.2</td>\n",
       "      <td>22.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44.5</td>\n",
       "      <td>39.3</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.2</td>\n",
       "      <td>45.9</td>\n",
       "      <td>69.3</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151.5</td>\n",
       "      <td>41.3</td>\n",
       "      <td>58.5</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>180.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>58.4</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>38.2</td>\n",
       "      <td>3.7</td>\n",
       "      <td>13.8</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>94.2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>8.1</td>\n",
       "      <td>9.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>177.0</td>\n",
       "      <td>9.3</td>\n",
       "      <td>6.4</td>\n",
       "      <td>12.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>283.6</td>\n",
       "      <td>42.0</td>\n",
       "      <td>66.2</td>\n",
       "      <td>25.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>232.1</td>\n",
       "      <td>8.6</td>\n",
       "      <td>8.7</td>\n",
       "      <td>13.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        TV  radio  newspaper  sales\n",
       "0    230.1   37.8       69.2   22.1\n",
       "1     44.5   39.3       45.1   10.4\n",
       "2     17.2   45.9       69.3    9.3\n",
       "3    151.5   41.3       58.5   18.5\n",
       "4    180.8   10.8       58.4   12.9\n",
       "..     ...    ...        ...    ...\n",
       "195   38.2    3.7       13.8    7.6\n",
       "196   94.2    4.9        8.1    9.7\n",
       "197  177.0    9.3        6.4   12.8\n",
       "198  283.6   42.0       66.2   25.5\n",
       "199  232.1    8.6        8.7   13.4\n",
       "\n",
       "[200 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Advertising.csv\")\n",
    "df = df.iloc[:, 1:]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8e0cdd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape :  (200, 4)\n",
      "Y Shape :  (200, 1)\n",
      "Thetas Shape :  (4, 1)\n"
     ]
    }
   ],
   "source": [
    "one = np.ones(200)\n",
    "X = df.iloc[: , :-1]\n",
    "X['One'] = one\n",
    "X = np.array(X)\n",
    "y = df.iloc[:,-1:]\n",
    "y = np.array(y)\n",
    "thetas = np.array([[100] , [50] , [75] , [1]])\n",
    "print(\"X Shape : \" , X.shape)\n",
    "print(\"Y Shape : \" , y.shape)\n",
    "print(\"Thetas Shape : \" , thetas.shape)\n",
    "alpha = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6c77c1fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[230.1,  37.8,  69.2,   1. ],\n",
       "        [ 44.5,  39.3,  45.1,   1. ],\n",
       "        [ 17.2,  45.9,  69.3,   1. ],\n",
       "        [151.5,  41.3,  58.5,   1. ],\n",
       "        [180.8,  10.8,  58.4,   1. ],\n",
       "        [  8.7,  48.9,  75. ,   1. ],\n",
       "        [ 57.5,  32.8,  23.5,   1. ],\n",
       "        [120.2,  19.6,  11.6,   1. ],\n",
       "        [  8.6,   2.1,   1. ,   1. ],\n",
       "        [199.8,   2.6,  21.2,   1. ],\n",
       "        [ 66.1,   5.8,  24.2,   1. ],\n",
       "        [214.7,  24. ,   4. ,   1. ],\n",
       "        [ 23.8,  35.1,  65.9,   1. ],\n",
       "        [ 97.5,   7.6,   7.2,   1. ],\n",
       "        [204.1,  32.9,  46. ,   1. ],\n",
       "        [195.4,  47.7,  52.9,   1. ],\n",
       "        [ 67.8,  36.6, 114. ,   1. ],\n",
       "        [281.4,  39.6,  55.8,   1. ],\n",
       "        [ 69.2,  20.5,  18.3,   1. ],\n",
       "        [147.3,  23.9,  19.1,   1. ],\n",
       "        [218.4,  27.7,  53.4,   1. ],\n",
       "        [237.4,   5.1,  23.5,   1. ],\n",
       "        [ 13.2,  15.9,  49.6,   1. ],\n",
       "        [228.3,  16.9,  26.2,   1. ],\n",
       "        [ 62.3,  12.6,  18.3,   1. ],\n",
       "        [262.9,   3.5,  19.5,   1. ],\n",
       "        [142.9,  29.3,  12.6,   1. ],\n",
       "        [240.1,  16.7,  22.9,   1. ],\n",
       "        [248.8,  27.1,  22.9,   1. ],\n",
       "        [ 70.6,  16. ,  40.8,   1. ],\n",
       "        [292.9,  28.3,  43.2,   1. ],\n",
       "        [112.9,  17.4,  38.6,   1. ],\n",
       "        [ 97.2,   1.5,  30. ,   1. ],\n",
       "        [265.6,  20. ,   0.3,   1. ],\n",
       "        [ 95.7,   1.4,   7.4,   1. ],\n",
       "        [290.7,   4.1,   8.5,   1. ],\n",
       "        [266.9,  43.8,   5. ,   1. ],\n",
       "        [ 74.7,  49.4,  45.7,   1. ],\n",
       "        [ 43.1,  26.7,  35.1,   1. ],\n",
       "        [228. ,  37.7,  32. ,   1. ],\n",
       "        [202.5,  22.3,  31.6,   1. ],\n",
       "        [177. ,  33.4,  38.7,   1. ],\n",
       "        [293.6,  27.7,   1.8,   1. ],\n",
       "        [206.9,   8.4,  26.4,   1. ],\n",
       "        [ 25.1,  25.7,  43.3,   1. ],\n",
       "        [175.1,  22.5,  31.5,   1. ],\n",
       "        [ 89.7,   9.9,  35.7,   1. ],\n",
       "        [239.9,  41.5,  18.5,   1. ],\n",
       "        [227.2,  15.8,  49.9,   1. ],\n",
       "        [ 66.9,  11.7,  36.8,   1. ],\n",
       "        [199.8,   3.1,  34.6,   1. ],\n",
       "        [100.4,   9.6,   3.6,   1. ],\n",
       "        [216.4,  41.7,  39.6,   1. ],\n",
       "        [182.6,  46.2,  58.7,   1. ],\n",
       "        [262.7,  28.8,  15.9,   1. ],\n",
       "        [198.9,  49.4,  60. ,   1. ],\n",
       "        [  7.3,  28.1,  41.4,   1. ],\n",
       "        [136.2,  19.2,  16.6,   1. ],\n",
       "        [210.8,  49.6,  37.7,   1. ],\n",
       "        [210.7,  29.5,   9.3,   1. ],\n",
       "        [ 53.5,   2. ,  21.4,   1. ],\n",
       "        [261.3,  42.7,  54.7,   1. ],\n",
       "        [239.3,  15.5,  27.3,   1. ],\n",
       "        [102.7,  29.6,   8.4,   1. ],\n",
       "        [131.1,  42.8,  28.9,   1. ],\n",
       "        [ 69. ,   9.3,   0.9,   1. ],\n",
       "        [ 31.5,  24.6,   2.2,   1. ],\n",
       "        [139.3,  14.5,  10.2,   1. ],\n",
       "        [237.4,  27.5,  11. ,   1. ],\n",
       "        [216.8,  43.9,  27.2,   1. ],\n",
       "        [199.1,  30.6,  38.7,   1. ],\n",
       "        [109.8,  14.3,  31.7,   1. ],\n",
       "        [ 26.8,  33. ,  19.3,   1. ],\n",
       "        [129.4,   5.7,  31.3,   1. ],\n",
       "        [213.4,  24.6,  13.1,   1. ],\n",
       "        [ 16.9,  43.7,  89.4,   1. ],\n",
       "        [ 27.5,   1.6,  20.7,   1. ],\n",
       "        [120.5,  28.5,  14.2,   1. ],\n",
       "        [  5.4,  29.9,   9.4,   1. ],\n",
       "        [116. ,   7.7,  23.1,   1. ],\n",
       "        [ 76.4,  26.7,  22.3,   1. ],\n",
       "        [239.8,   4.1,  36.9,   1. ],\n",
       "        [ 75.3,  20.3,  32.5,   1. ],\n",
       "        [ 68.4,  44.5,  35.6,   1. ],\n",
       "        [213.5,  43. ,  33.8,   1. ],\n",
       "        [193.2,  18.4,  65.7,   1. ],\n",
       "        [ 76.3,  27.5,  16. ,   1. ],\n",
       "        [110.7,  40.6,  63.2,   1. ],\n",
       "        [ 88.3,  25.5,  73.4,   1. ],\n",
       "        [109.8,  47.8,  51.4,   1. ],\n",
       "        [134.3,   4.9,   9.3,   1. ],\n",
       "        [ 28.6,   1.5,  33. ,   1. ],\n",
       "        [217.7,  33.5,  59. ,   1. ],\n",
       "        [250.9,  36.5,  72.3,   1. ],\n",
       "        [107.4,  14. ,  10.9,   1. ],\n",
       "        [163.3,  31.6,  52.9,   1. ],\n",
       "        [197.6,   3.5,   5.9,   1. ],\n",
       "        [184.9,  21. ,  22. ,   1. ],\n",
       "        [289.7,  42.3,  51.2,   1. ],\n",
       "        [135.2,  41.7,  45.9,   1. ],\n",
       "        [222.4,   4.3,  49.8,   1. ],\n",
       "        [296.4,  36.3, 100.9,   1. ],\n",
       "        [280.2,  10.1,  21.4,   1. ],\n",
       "        [187.9,  17.2,  17.9,   1. ],\n",
       "        [238.2,  34.3,   5.3,   1. ],\n",
       "        [137.9,  46.4,  59. ,   1. ],\n",
       "        [ 25. ,  11. ,  29.7,   1. ],\n",
       "        [ 90.4,   0.3,  23.2,   1. ],\n",
       "        [ 13.1,   0.4,  25.6,   1. ],\n",
       "        [255.4,  26.9,   5.5,   1. ],\n",
       "        [225.8,   8.2,  56.5,   1. ],\n",
       "        [241.7,  38. ,  23.2,   1. ],\n",
       "        [175.7,  15.4,   2.4,   1. ],\n",
       "        [209.6,  20.6,  10.7,   1. ],\n",
       "        [ 78.2,  46.8,  34.5,   1. ],\n",
       "        [ 75.1,  35. ,  52.7,   1. ],\n",
       "        [139.2,  14.3,  25.6,   1. ],\n",
       "        [ 76.4,   0.8,  14.8,   1. ],\n",
       "        [125.7,  36.9,  79.2,   1. ],\n",
       "        [ 19.4,  16. ,  22.3,   1. ],\n",
       "        [141.3,  26.8,  46.2,   1. ],\n",
       "        [ 18.8,  21.7,  50.4,   1. ],\n",
       "        [224. ,   2.4,  15.6,   1. ],\n",
       "        [123.1,  34.6,  12.4,   1. ],\n",
       "        [229.5,  32.3,  74.2,   1. ],\n",
       "        [ 87.2,  11.8,  25.9,   1. ],\n",
       "        [  7.8,  38.9,  50.6,   1. ],\n",
       "        [ 80.2,   0. ,   9.2,   1. ],\n",
       "        [220.3,  49. ,   3.2,   1. ],\n",
       "        [ 59.6,  12. ,  43.1,   1. ],\n",
       "        [  0.7,  39.6,   8.7,   1. ],\n",
       "        [265.2,   2.9,  43. ,   1. ],\n",
       "        [  8.4,  27.2,   2.1,   1. ],\n",
       "        [219.8,  33.5,  45.1,   1. ],\n",
       "        [ 36.9,  38.6,  65.6,   1. ],\n",
       "        [ 48.3,  47. ,   8.5,   1. ],\n",
       "        [ 25.6,  39. ,   9.3,   1. ],\n",
       "        [273.7,  28.9,  59.7,   1. ],\n",
       "        [ 43. ,  25.9,  20.5,   1. ],\n",
       "        [184.9,  43.9,   1.7,   1. ],\n",
       "        [ 73.4,  17. ,  12.9,   1. ],\n",
       "        [193.7,  35.4,  75.6,   1. ],\n",
       "        [220.5,  33.2,  37.9,   1. ],\n",
       "        [104.6,   5.7,  34.4,   1. ],\n",
       "        [ 96.2,  14.8,  38.9,   1. ],\n",
       "        [140.3,   1.9,   9. ,   1. ],\n",
       "        [240.1,   7.3,   8.7,   1. ],\n",
       "        [243.2,  49. ,  44.3,   1. ],\n",
       "        [ 38. ,  40.3,  11.9,   1. ],\n",
       "        [ 44.7,  25.8,  20.6,   1. ],\n",
       "        [280.7,  13.9,  37. ,   1. ],\n",
       "        [121. ,   8.4,  48.7,   1. ],\n",
       "        [197.6,  23.3,  14.2,   1. ],\n",
       "        [171.3,  39.7,  37.7,   1. ],\n",
       "        [187.8,  21.1,   9.5,   1. ],\n",
       "        [  4.1,  11.6,   5.7,   1. ],\n",
       "        [ 93.9,  43.5,  50.5,   1. ],\n",
       "        [149.8,   1.3,  24.3,   1. ],\n",
       "        [ 11.7,  36.9,  45.2,   1. ],\n",
       "        [131.7,  18.4,  34.6,   1. ],\n",
       "        [172.5,  18.1,  30.7,   1. ],\n",
       "        [ 85.7,  35.8,  49.3,   1. ],\n",
       "        [188.4,  18.1,  25.6,   1. ],\n",
       "        [163.5,  36.8,   7.4,   1. ],\n",
       "        [117.2,  14.7,   5.4,   1. ],\n",
       "        [234.5,   3.4,  84.8,   1. ],\n",
       "        [ 17.9,  37.6,  21.6,   1. ],\n",
       "        [206.8,   5.2,  19.4,   1. ],\n",
       "        [215.4,  23.6,  57.6,   1. ],\n",
       "        [284.3,  10.6,   6.4,   1. ],\n",
       "        [ 50. ,  11.6,  18.4,   1. ],\n",
       "        [164.5,  20.9,  47.4,   1. ],\n",
       "        [ 19.6,  20.1,  17. ,   1. ],\n",
       "        [168.4,   7.1,  12.8,   1. ],\n",
       "        [222.4,   3.4,  13.1,   1. ],\n",
       "        [276.9,  48.9,  41.8,   1. ],\n",
       "        [248.4,  30.2,  20.3,   1. ],\n",
       "        [170.2,   7.8,  35.2,   1. ],\n",
       "        [276.7,   2.3,  23.7,   1. ],\n",
       "        [165.6,  10. ,  17.6,   1. ],\n",
       "        [156.6,   2.6,   8.3,   1. ],\n",
       "        [218.5,   5.4,  27.4,   1. ],\n",
       "        [ 56.2,   5.7,  29.7,   1. ],\n",
       "        [287.6,  43. ,  71.8,   1. ],\n",
       "        [253.8,  21.3,  30. ,   1. ],\n",
       "        [205. ,  45.1,  19.6,   1. ],\n",
       "        [139.5,   2.1,  26.6,   1. ],\n",
       "        [191.1,  28.7,  18.2,   1. ],\n",
       "        [286. ,  13.9,   3.7,   1. ],\n",
       "        [ 18.7,  12.1,  23.4,   1. ],\n",
       "        [ 39.5,  41.1,   5.8,   1. ],\n",
       "        [ 75.5,  10.8,   6. ,   1. ],\n",
       "        [ 17.2,   4.1,  31.6,   1. ],\n",
       "        [166.8,  42. ,   3.6,   1. ],\n",
       "        [149.7,  35.6,   6. ,   1. ],\n",
       "        [ 38.2,   3.7,  13.8,   1. ],\n",
       "        [ 94.2,   4.9,   8.1,   1. ],\n",
       "        [177. ,   9.3,   6.4,   1. ],\n",
       "        [283.6,  42. ,  66.2,   1. ],\n",
       "        [232.1,   8.6,   8.7,   1. ]]),\n",
       " array([[22.1],\n",
       "        [10.4],\n",
       "        [ 9.3],\n",
       "        [18.5],\n",
       "        [12.9],\n",
       "        [ 7.2],\n",
       "        [11.8],\n",
       "        [13.2],\n",
       "        [ 4.8],\n",
       "        [10.6],\n",
       "        [ 8.6],\n",
       "        [17.4],\n",
       "        [ 9.2],\n",
       "        [ 9.7],\n",
       "        [19. ],\n",
       "        [22.4],\n",
       "        [12.5],\n",
       "        [24.4],\n",
       "        [11.3],\n",
       "        [14.6],\n",
       "        [18. ],\n",
       "        [12.5],\n",
       "        [ 5.6],\n",
       "        [15.5],\n",
       "        [ 9.7],\n",
       "        [12. ],\n",
       "        [15. ],\n",
       "        [15.9],\n",
       "        [18.9],\n",
       "        [10.5],\n",
       "        [21.4],\n",
       "        [11.9],\n",
       "        [ 9.6],\n",
       "        [17.4],\n",
       "        [ 9.5],\n",
       "        [12.8],\n",
       "        [25.4],\n",
       "        [14.7],\n",
       "        [10.1],\n",
       "        [21.5],\n",
       "        [16.6],\n",
       "        [17.1],\n",
       "        [20.7],\n",
       "        [12.9],\n",
       "        [ 8.5],\n",
       "        [14.9],\n",
       "        [10.6],\n",
       "        [23.2],\n",
       "        [14.8],\n",
       "        [ 9.7],\n",
       "        [11.4],\n",
       "        [10.7],\n",
       "        [22.6],\n",
       "        [21.2],\n",
       "        [20.2],\n",
       "        [23.7],\n",
       "        [ 5.5],\n",
       "        [13.2],\n",
       "        [23.8],\n",
       "        [18.4],\n",
       "        [ 8.1],\n",
       "        [24.2],\n",
       "        [15.7],\n",
       "        [14. ],\n",
       "        [18. ],\n",
       "        [ 9.3],\n",
       "        [ 9.5],\n",
       "        [13.4],\n",
       "        [18.9],\n",
       "        [22.3],\n",
       "        [18.3],\n",
       "        [12.4],\n",
       "        [ 8.8],\n",
       "        [11. ],\n",
       "        [17. ],\n",
       "        [ 8.7],\n",
       "        [ 6.9],\n",
       "        [14.2],\n",
       "        [ 5.3],\n",
       "        [11. ],\n",
       "        [11.8],\n",
       "        [12.3],\n",
       "        [11.3],\n",
       "        [13.6],\n",
       "        [21.7],\n",
       "        [15.2],\n",
       "        [12. ],\n",
       "        [16. ],\n",
       "        [12.9],\n",
       "        [16.7],\n",
       "        [11.2],\n",
       "        [ 7.3],\n",
       "        [19.4],\n",
       "        [22.2],\n",
       "        [11.5],\n",
       "        [16.9],\n",
       "        [11.7],\n",
       "        [15.5],\n",
       "        [25.4],\n",
       "        [17.2],\n",
       "        [11.7],\n",
       "        [23.8],\n",
       "        [14.8],\n",
       "        [14.7],\n",
       "        [20.7],\n",
       "        [19.2],\n",
       "        [ 7.2],\n",
       "        [ 8.7],\n",
       "        [ 5.3],\n",
       "        [19.8],\n",
       "        [13.4],\n",
       "        [21.8],\n",
       "        [14.1],\n",
       "        [15.9],\n",
       "        [14.6],\n",
       "        [12.6],\n",
       "        [12.2],\n",
       "        [ 9.4],\n",
       "        [15.9],\n",
       "        [ 6.6],\n",
       "        [15.5],\n",
       "        [ 7. ],\n",
       "        [11.6],\n",
       "        [15.2],\n",
       "        [19.7],\n",
       "        [10.6],\n",
       "        [ 6.6],\n",
       "        [ 8.8],\n",
       "        [24.7],\n",
       "        [ 9.7],\n",
       "        [ 1.6],\n",
       "        [12.7],\n",
       "        [ 5.7],\n",
       "        [19.6],\n",
       "        [10.8],\n",
       "        [11.6],\n",
       "        [ 9.5],\n",
       "        [20.8],\n",
       "        [ 9.6],\n",
       "        [20.7],\n",
       "        [10.9],\n",
       "        [19.2],\n",
       "        [20.1],\n",
       "        [10.4],\n",
       "        [11.4],\n",
       "        [10.3],\n",
       "        [13.2],\n",
       "        [25.4],\n",
       "        [10.9],\n",
       "        [10.1],\n",
       "        [16.1],\n",
       "        [11.6],\n",
       "        [16.6],\n",
       "        [19. ],\n",
       "        [15.6],\n",
       "        [ 3.2],\n",
       "        [15.3],\n",
       "        [10.1],\n",
       "        [ 7.3],\n",
       "        [12.9],\n",
       "        [14.4],\n",
       "        [13.3],\n",
       "        [14.9],\n",
       "        [18. ],\n",
       "        [11.9],\n",
       "        [11.9],\n",
       "        [ 8. ],\n",
       "        [12.2],\n",
       "        [17.1],\n",
       "        [15. ],\n",
       "        [ 8.4],\n",
       "        [14.5],\n",
       "        [ 7.6],\n",
       "        [11.7],\n",
       "        [11.5],\n",
       "        [27. ],\n",
       "        [20.2],\n",
       "        [11.7],\n",
       "        [11.8],\n",
       "        [12.6],\n",
       "        [10.5],\n",
       "        [12.2],\n",
       "        [ 8.7],\n",
       "        [26.2],\n",
       "        [17.6],\n",
       "        [22.6],\n",
       "        [10.3],\n",
       "        [17.3],\n",
       "        [15.9],\n",
       "        [ 6.7],\n",
       "        [10.8],\n",
       "        [ 9.9],\n",
       "        [ 5.9],\n",
       "        [19.6],\n",
       "        [17.3],\n",
       "        [ 7.6],\n",
       "        [ 9.7],\n",
       "        [12.8],\n",
       "        [25.5],\n",
       "        [13.4]]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1af95af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30091. ],\n",
       "       [ 9798.5],\n",
       "       [ 9213.5],\n",
       "       [21603.5],\n",
       "       [23001. ],\n",
       "       [ 8941. ],\n",
       "       [ 9153.5],\n",
       "       [13871. ],\n",
       "       [ 1041. ],\n",
       "       [21701. ],\n",
       "       [ 8716. ],\n",
       "       [22971. ],\n",
       "       [ 9078.5],\n",
       "       [10671. ],\n",
       "       [25506. ],\n",
       "       [25893.5],\n",
       "       [17161. ],\n",
       "       [34306. ],\n",
       "       [ 9318.5],\n",
       "       [17358.5],\n",
       "       [27231. ],\n",
       "       [25758.5],\n",
       "       [ 5836. ],\n",
       "       [25641. ],\n",
       "       [ 8233.5],\n",
       "       [27928.5],\n",
       "       [16701. ],\n",
       "       [26563.5],\n",
       "       [27953.5],\n",
       "       [10921. ],\n",
       "       [33946. ],\n",
       "       [15056. ],\n",
       "       [12046. ],\n",
       "       [27583.5],\n",
       "       [10196. ],\n",
       "       [29913.5],\n",
       "       [29256. ],\n",
       "       [13368.5],\n",
       "       [ 8278.5],\n",
       "       [27086. ],\n",
       "       [23736. ],\n",
       "       [22273.5],\n",
       "       [30881. ],\n",
       "       [23091. ],\n",
       "       [ 7043.5],\n",
       "       [20998.5],\n",
       "       [12143.5],\n",
       "       [27453.5],\n",
       "       [27253.5],\n",
       "       [10036. ],\n",
       "       [22731. ],\n",
       "       [10791. ],\n",
       "       [26696. ],\n",
       "       [24973.5],\n",
       "       [28903.5],\n",
       "       [26861. ],\n",
       "       [ 5241. ],\n",
       "       [15826. ],\n",
       "       [26388.5],\n",
       "       [23243.5],\n",
       "       [ 7056. ],\n",
       "       [32368.5],\n",
       "       [26753.5],\n",
       "       [12381. ],\n",
       "       [17418.5],\n",
       "       [ 7433.5],\n",
       "       [ 4546. ],\n",
       "       [15421. ],\n",
       "       [25941. ],\n",
       "       [25916. ],\n",
       "       [24343.5],\n",
       "       [14073.5],\n",
       "       [ 5778.5],\n",
       "       [15573.5],\n",
       "       [23553.5],\n",
       "       [10581. ],\n",
       "       [ 4383.5],\n",
       "       [14541. ],\n",
       "       [ 2741. ],\n",
       "       [13718.5],\n",
       "       [10648.5],\n",
       "       [26953.5],\n",
       "       [10983.5],\n",
       "       [11736. ],\n",
       "       [26036. ],\n",
       "       [25168.5],\n",
       "       [10206. ],\n",
       "       [17841. ],\n",
       "       [15611. ],\n",
       "       [17226. ],\n",
       "       [14373.5],\n",
       "       [ 5411. ],\n",
       "       [27871. ],\n",
       "       [32338.5],\n",
       "       [12258.5],\n",
       "       [21878.5],\n",
       "       [20378.5],\n",
       "       [21191. ],\n",
       "       [34926. ],\n",
       "       [19048.5],\n",
       "       [26191. ],\n",
       "       [39023.5],\n",
       "       [30131. ],\n",
       "       [20993.5],\n",
       "       [25933.5],\n",
       "       [20536. ],\n",
       "       [ 5278.5],\n",
       "       [10796. ],\n",
       "       [ 3251. ],\n",
       "       [27298.5],\n",
       "       [27228.5],\n",
       "       [27811. ],\n",
       "       [18521. ],\n",
       "       [22793.5],\n",
       "       [12748.5],\n",
       "       [13213.5],\n",
       "       [16556. ],\n",
       "       [ 8791. ],\n",
       "       [20356. ],\n",
       "       [ 4413.5],\n",
       "       [18936. ],\n",
       "       [ 6746. ],\n",
       "       [23691. ],\n",
       "       [14971. ],\n",
       "       [30131. ],\n",
       "       [11253.5],\n",
       "       [ 6521. ],\n",
       "       [ 8711. ],\n",
       "       [24721. ],\n",
       "       [ 9793.5],\n",
       "       [ 2703.5],\n",
       "       [29891. ],\n",
       "       [ 2358.5],\n",
       "       [27038.5],\n",
       "       [10541. ],\n",
       "       [ 7818.5],\n",
       "       [ 5208.5],\n",
       "       [33293.5],\n",
       "       [ 7133.5],\n",
       "       [20813.5],\n",
       "       [ 9158.5],\n",
       "       [26811. ],\n",
       "       [26553.5],\n",
       "       [13326. ],\n",
       "       [13278.5],\n",
       "       [14801. ],\n",
       "       [25028.5],\n",
       "       [30093.5],\n",
       "       [ 6708.5],\n",
       "       [ 7306. ],\n",
       "       [31541. ],\n",
       "       [16173.5],\n",
       "       [21991. ],\n",
       "       [21943.5],\n",
       "       [20548.5],\n",
       "       [ 1418.5],\n",
       "       [15353.5],\n",
       "       [16868.5],\n",
       "       [ 6406. ],\n",
       "       [16686. ],\n",
       "       [20458.5],\n",
       "       [14058.5],\n",
       "       [21666. ],\n",
       "       [18746. ],\n",
       "       [12861. ],\n",
       "       [29981. ],\n",
       "       [ 5291. ],\n",
       "       [22396. ],\n",
       "       [27041. ],\n",
       "       [29441. ],\n",
       "       [ 6961. ],\n",
       "       [21051. ],\n",
       "       [ 4241. ],\n",
       "       [18156. ],\n",
       "       [23393.5],\n",
       "       [33271. ],\n",
       "       [27873.5],\n",
       "       [20051. ],\n",
       "       [29563.5],\n",
       "       [18381. ],\n",
       "       [16413.5],\n",
       "       [24176. ],\n",
       "       [ 8133.5],\n",
       "       [36296. ],\n",
       "       [28696. ],\n",
       "       [24226. ],\n",
       "       [16051. ],\n",
       "       [21911. ],\n",
       "       [29573.5],\n",
       "       [ 4231. ],\n",
       "       [ 6441. ],\n",
       "       [ 8541. ],\n",
       "       [ 4296. ],\n",
       "       [19051. ],\n",
       "       [17201. ],\n",
       "       [ 5041. ],\n",
       "       [10273.5],\n",
       "       [18646. ],\n",
       "       [35426. ],\n",
       "       [24293.5]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = hypothesis(X, thetas)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "488d35d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "408872809.05125"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE = loss_fun(y, y_pred)\n",
    "MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "76eac5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6825341.3416],\n",
       "       [ 897131.0866],\n",
       "       [1212073.7077],\n",
       "       [  36291.955 ]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_thetas = der_loss_fun(X, y, thetas)\n",
    "grad_thetas = grad_thetas.T\n",
    "grad_thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "454b49cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-582.53413416],\n",
       "       [ -39.71310866],\n",
       "       [ -46.20737077],\n",
       "       [  -2.6291955 ]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nthetas = gradient_descent(thetas, alpha, grad_thetas)\n",
    "nthetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "19e6ddcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  1 Loop :  10245565150.43966\n",
      "Loss for  2 Loop :  258935092657.69727\n",
      "Loss for  3 Loop :  6545610952253.798\n",
      "Loss for  4 Loop :  165467377974489.84\n",
      "Loss for  5 Loop :  4182872848598497.5\n",
      "Loss for  6 Loop :  1.0573942473738133e+17\n",
      "Loss for  7 Loop :  2.6730016307310413e+18\n",
      "Loss for  8 Loop :  6.757118015031614e+19\n",
      "Loss for  9 Loop :  1.7081412650160958e+21\n",
      "Loss for  10 Loop :  4.318034071271358e+22\n",
      "Loss for  11 Loop :  1.0915618410801986e+24\n",
      "Loss for  12 Loop :  2.759374366287891e+25\n",
      "Loss for  13 Loop :  6.975460855055007e+26\n",
      "Loss for  14 Loop :  1.7633364553524404e+28\n",
      "Loss for  15 Loop :  4.457562760920101e+29\n",
      "Loss for  16 Loop :  1.126833492679661e+31\n",
      "Loss for  17 Loop :  2.8485380651433575e+32\n",
      "Loss for  18 Loop :  7.200859010034219e+33\n",
      "Loss for  19 Loop :  1.820315168573372e+35\n",
      "Loss for  20 Loop :  4.601600042885104e+36\n",
      "Loss for  21 Loop :  1.1632448776041003e+38\n",
      "Loss for  22 Loop :  2.940582911729522e+39\n",
      "Loss for  23 Loop :  7.433540458450756e+40\n",
      "Loss for  24 Loop :  1.879135035676455e+42\n",
      "Loss for  25 Loop :  4.750291603367001e+43\n",
      "Loss for  26 Loop :  1.2008328240708863e+45\n",
      "Loss for  27 Loop :  3.035602004609513e+46\n",
      "Loss for  28 Loop :  7.673740545457749e+47\n",
      "Loss for  29 Loop :  1.93985554989041e+49\n",
      "Loss for  30 Loop :  4.9037878361264775e+50\n",
      "Loss for  31 Loop :  1.2396353503280435e+52\n",
      "Loss for  32 Loop :  3.133691450641499e+53\n",
      "Loss for  33 Loop :  7.921702220919208e+54\n",
      "Loss for  34 Loop :  2.002538126849408e+56\n",
      "Loss for  35 Loop :  5.062243994599702e+57\n",
      "Loss for  36 Loop :  1.2796917031077256e+59\n",
      "Loss for  37 Loop :  3.234950462185783e+60\n",
      "Loss for  38 Loop :  8.177676285141208e+61\n",
      "Loss for  39 Loop :  2.0672461667117898e+63\n",
      "Loss for  40 Loop :  5.225820348929103e+64\n",
      "Loss for  41 Loop :  1.3210423973222382e+66\n",
      "Loss for  42 Loop :  3.339481457452914e+67\n",
      "Loss for  43 Loop :  8.441921642543259e+68\n",
      "Loss for  44 Loop :  2.1340451182860077e+70\n",
      "Loss for  45 Loop :  5.394682348068244e+71\n",
      "Loss for  46 Loop :  1.3637292570427617e+73\n",
      "Loss for  47 Loop :  3.447390164094377e+74\n",
      "Loss for  48 Loop :  8.714705563526678e+75\n",
      "Loss for  49 Loop :  2.2030025452286956e+77\n",
      "Loss for  50 Loop :  5.569000787124819e+78\n",
      "Loss for  51 Loop :  1.40779545780222e+80\n",
      "Loss for  52 Loop :  3.5587857261406057e+81\n",
      "Loss for  53 Loop :  8.996303954804641e+82\n",
      "Loss for  54 Loop :  2.274188194381781e+84\n",
      "Loss for  55 Loop :  5.748951980111011e+85\n",
      "Loss for  56 Loop :  1.453285570265077e+87\n",
      "Loss for  57 Loop :  3.673780814394464e+88\n",
      "Loss for  58 Loop :  9.287001638467434e+89\n",
      "Loss for  59 Loop :  2.3476740663177775e+91\n",
      "Loss for  60 Loop :  5.934717938275909e+92\n",
      "Loss for  61 Loop :  1.5002456053082468e+94\n",
      "Loss for  62 Loop :  3.792491740391908e+95\n",
      "Loss for  63 Loop :  9.5870926400652e+96\n",
      "Loss for  64 Loop :  2.423534488164613e+98\n",
      "Loss for  65 Loop :  6.126486554200407e+99\n",
      "Loss for  66 Loop :  1.54872306055869e+101\n",
      "Loss for  67 Loop :  3.9150385740450195e+102\n",
      "Loss for  68 Loop :  9.89688048599933e+103\n",
      "Loss for  69 Loop :  2.5018461887836335e+105\n",
      "Loss for  70 Loop :  6.324451791840721e+106\n",
      "Loss for  71 Loop :  1.598766968434784e+108\n",
      "Loss for  72 Loop :  4.041545265086468e+109\n",
      "Loss for  73 Loop :  1.0216678510523749e+111\n",
      "Loss for  74 Loop :  2.5826883763768634e+112\n",
      "Loss for  75 Loop :  6.528813882712863e+113\n",
      "Loss for  76 Loop :  1.650427945740068e+115\n",
      "Loss for  77 Loop :  4.172139768438211e+116\n",
      "Loss for  78 Loop :  1.054681017266603e+118\n",
      "Loss for  79 Loop :  2.6661428186019552e+119\n",
      "Loss for  80 Loop :  6.73977952841635e+120\n",
      "Loss for  81 Loop :  1.703758244859494e+122\n",
      "Loss for  82 Loop :  4.306954173631216e+123\n",
      "Loss for  83 Loop :  1.088760938338945e+125\n",
      "Loss for  84 Loop :  2.752293925275926e+126\n",
      "Loss for  85 Loop :  6.957562109702113e+127\n",
      "Loss for  86 Loop :  1.7588118066099902e+129\n",
      "Loss for  87 Loop :  4.446124838407145e+130\n",
      "Loss for  88 Loop :  1.1239420843326437e+132\n",
      "Loss for  89 Loop :  2.841228833751271e+133\n",
      "Loss for  90 Loop :  7.1823819022959735e+134\n",
      "Loss for  91 Loop :  1.815644314798785e+136\n",
      "Loss for  92 Loop :  4.589792526637082e+137\n",
      "Loss for  93 Loop :  1.1602600391425363e+139\n",
      "Loss for  94 Loop :  2.9330374970508663e+140\n",
      "Loss for  95 Loop :  7.414466299696094e+141\n",
      "Loss for  96 Loop :  1.874313252544678e+143\n",
      "Loss for  97 Loop :  4.7381025506968756e+144\n",
      "Loss for  98 Loop :  1.1977515364863013e+146\n",
      "Loss for  99 Loop :  3.0278127748507555e+147\n",
      "Loss for  100 Loop :  7.654050043169597e+148\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    y_pred = hypothesis(X, nthetas)\n",
    "    grad_thetas = der_loss_fun(X, y, nthetas)\n",
    "    grad_thetas = grad_thetas.T\n",
    "    nthetas = gradient_descent(nthetas, alpha, grad_thetas)\n",
    "    print(\"Loss for \", i +1, \"Loop : \", loss_fun(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "18da031e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4362.46562742]\n",
      " [11222.94696351]\n",
      " [18196.08239048]\n",
      " [  226.40975108]]\n"
     ]
    }
   ],
   "source": [
    "print(grad_thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a071996a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100]\n",
      " [ 50]\n",
      " [ 75]\n",
      " [  1]]\n"
     ]
    }
   ],
   "source": [
    "#original value of thetas\n",
    "print(thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "266e5da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Therefore, the original thetas and final thetas show a linear relationship between features and outputs..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0aac92",
   "metadata": {},
   "source": [
    "# Task-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "052abcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1,2,3,4,5] , [1,1,1,1,1]]).T\n",
    "y = np.array([[3],[5],[7],[9],[11]])\n",
    "nthetas = np.array([[10] , [1]])\n",
    "thetas = nthetas\n",
    "alpha = 0.066"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bf46ebc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for  1 Loop :  704.0\n",
      "Loss for  2 Loop :  222.59916799999996\n",
      "Loss for  3 Loop :  70.699863179264\n",
      "Loss for  4 Loop :  22.756210163547394\n",
      "Loss for  5 Loop :  7.610524733619438\n",
      "Loss for  6 Loop :  2.81317377962013\n",
      "Loss for  7 Loop :  1.2814665092507096\n",
      "Loss for  8 Loop :  0.7808422984030051\n",
      "Loss for  9 Loop :  0.6062824759014408\n",
      "Loss for  10 Loop :  0.5353263014842767\n",
      "Loss for  11 Loop :  0.49775710200823164\n",
      "Loss for  12 Loop :  0.4713912304289316\n",
      "Loss for  13 Loop :  0.4492003812462668\n",
      "Loss for  14 Loop :  0.4289389242569266\n",
      "Loss for  15 Loop :  0.40987136651012557\n",
      "Loss for  16 Loop :  0.39173984171544424\n",
      "Loss for  17 Loop :  0.3744383100645754\n",
      "Loss for  18 Loop :  0.3579097209874419\n",
      "Loss for  19 Loop :  0.3421135208756017\n",
      "Loss for  20 Loop :  0.3270153561151292\n",
      "Loss for  21 Loop :  0.3125837805627321\n",
      "Loss for  22 Loop :  0.29878917479754247\n",
      "Loss for  23 Loop :  0.285603365020639\n",
      "Loss for  24 Loop :  0.2729994644581434\n",
      "Loss for  25 Loop :  0.2609517866551304\n",
      "Loss for  26 Loop :  0.24943578306871847\n",
      "Loss for  27 Loop :  0.23842798987418723\n",
      "Loss for  28 Loop :  0.2279059791563363\n",
      "Loss for  29 Loop :  0.21784831289769152\n",
      "Loss for  30 Loop :  0.20823449920007348\n",
      "Loss for  31 Loop :  0.19904495050213108\n",
      "Loss for  32 Loop :  0.19026094366095947\n",
      "Loss for  33 Loop :  0.18186458180171158\n",
      "Loss for  34 Loop :  0.17383875785282482\n",
      "Loss for  35 Loop :  0.16616711969110615\n",
      "Loss for  36 Loop :  0.1588340368251841\n",
      "Loss for  37 Loop :  0.15182456854931395\n",
      "Loss for  38 Loop :  0.14512443350259674\n",
      "Loss for  39 Loop :  0.13871998057158244\n",
      "Loss for  40 Loop :  0.13259816107696196\n",
      "Loss for  41 Loop :  0.12674650218768685\n",
      "Loss for  42 Loop :  0.12115308150834117\n",
      "Loss for  43 Loop :  0.11580650278799318\n",
      "Loss for  44 Loop :  0.11069587270103558\n",
      "Loss for  45 Loop :  0.10581077865270205\n",
      "Loss for  46 Loop :  0.10114126756404626\n",
      "Loss for  47 Loop :  0.0966778255931568\n",
      "Loss for  48 Loop :  0.09241135875128552\n",
      "Loss for  49 Loop :  0.08833317437440637\n",
      "Loss for  50 Loop :  0.08443496341244723\n",
      "Loss for  51 Loop :  0.0807087835001083\n",
      "Loss for  52 Loop :  0.0771470427747836\n",
      "Loss for  53 Loop :  0.07374248440860585\n",
      "Loss for  54 Loop :  0.07048817182310656\n",
      "Loss for  55 Loop :  0.06737747455636238\n",
      "Loss for  56 Loop :  0.064404054753837\n",
      "Loss for  57 Loop :  0.061561854255393636\n",
      "Loss for  58 Loop :  0.05884508225216249\n",
      "Loss for  59 Loop :  0.056248203488126466\n",
      "Loss for  60 Loop :  0.0537659269823761\n",
      "Loss for  61 Loop :  0.05139319524906125\n",
      "Loss for  62 Loop :  0.04912517399307354\n",
      "Loss for  63 Loop :  0.04695724226046903\n",
      "Loss for  64 Loop :  0.04488498302355687\n",
      "Loss for  65 Loop :  0.04290417418147709\n",
      "Loss for  66 Loop :  0.04101077995793041\n",
      "Loss for  67 Loop :  0.039200942678530595\n",
      "Loss for  68 Loop :  0.03747097491103158\n",
      "Loss for  69 Loop :  0.03581735195240962\n",
      "Loss for  70 Loop :  0.034236704647497285\n",
      "Loss for  71 Loop :  0.03272581252453855\n",
      "Loss for  72 Loop :  0.03128159723367327\n",
      "Loss for  73 Loop :  0.029901116274990223\n",
      "Loss for  74 Loop :  0.0285815570033632\n",
      "Loss for  75 Loop :  0.02732023089786027\n",
      "Loss for  76 Loop :  0.026114568084047086\n",
      "Loss for  77 Loop :  0.024962112098025655\n",
      "Loss for  78 Loop :  0.02386051488153994\n",
      "Loss for  79 Loop :  0.022807531997952296\n",
      "Loss for  80 Loop :  0.021801018059340628\n",
      "Loss for  81 Loop :  0.020838922355402988\n",
      "Loss for  82 Loop :  0.01991928467526107\n",
      "Loss for  83 Loop :  0.019040231313651085\n",
      "Loss for  84 Loop :  0.0181999712533648\n",
      "Loss for  85 Loop :  0.017396792516161316\n",
      "Loss for  86 Loop :  0.01662905867471708\n",
      "Loss for  87 Loop :  0.01589520551850531\n",
      "Loss for  88 Loop :  0.015193737866814178\n",
      "Loss for  89 Loop :  0.014523226522406812\n",
      "Loss for  90 Loop :  0.013882305359620522\n",
      "Loss for  91 Loop :  0.013269668540968829\n",
      "Loss for  92 Loop :  0.012684067856578973\n",
      "Loss for  93 Loop :  0.012124310181040583\n",
      "Loss for  94 Loop :  0.011589255042485197\n",
      "Loss for  95 Loop :  0.011077812298946225\n",
      "Loss for  96 Loop :  0.010588939917260324\n",
      "Loss for  97 Loop :  0.010121641849990268\n",
      "Loss for  98 Loop :  0.009674966006038146\n",
      "Loss for  99 Loop :  0.009248002310819075\n",
      "Loss for  100 Loop :  0.008839880852040159\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    y_pred = hypothesis(X , nthetas)\n",
    "    grad_thetas = der_loss_fun(X, y, nthetas)\n",
    "    grad_thetas = grad_thetas.T\n",
    "    nthetas = gradient_descent (nthetas, alpha, grad_thetas)\n",
    "    print(\"Loss for \", i +1, \"Loop : \", loss_fun(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "07756cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.05967886],\n",
       "       [0.78454039]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nthetas # updated new thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5cce83a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10],\n",
       "       [ 1]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetas  #original thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e2481a",
   "metadata": {},
   "source": [
    "# Task-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f330be7",
   "metadata": {},
   "source": [
    "Question: Is linear regression good for classification? If yes, then why, and if no, then why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f0e3f7",
   "metadata": {},
   "source": [
    "Answer: Linear regression is not suitable because it only works for continous values whereas classification problems are concerned with discret values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fd3711",
   "metadata": {},
   "source": [
    "# Task-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9fabef",
   "metadata": {},
   "source": [
    "Question: Why do we use mse (mean squared error) over squared error? What is the benefit of mse?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdd0f45",
   "metadata": {},
   "source": [
    "Answer: MSE calculates the average squared difference between the actual and predicted values. squared error will not show a mean of all errors rather just sum up all squared errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edbe754",
   "metadata": {},
   "source": [
    "# Task-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa9d579",
   "metadata": {},
   "source": [
    "Question: Can you find a better estimator than mse?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9635aa",
   "metadata": {},
   "source": [
    "Answer: Yes. we can consider RMSE..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
